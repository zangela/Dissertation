objdef[i,j]=1
}
}
}
#OBJDEF MATRICE CON VALORI DI PRESENZA PER L'OGGETTO
##################################################################
#
#
#
#
#creo la matrice finale dei domini dei sender che conta la presenza dei singoli domini
#
#
#
#
##################################################################
#ottengo tutti i dominii
len = 0
alldom=datidef[,8]
#dominii in ordine alfabetico
alldom= sort(alldom)
#conto e mi salvo tutti i dominii diversi
tmpdom = character(nrow(datidef))
idx=1
tmpdom[idx]=toString(alldom[idx])
len=1
for (i in 2:nrow(datidef))
{
if (tmpdom[idx]!=toString(alldom[i]))
{
len = len +1
idx=idx+1
tmpdom[idx]=toString(alldom[i])
}
}
#creo un vettore che contiene tutti i dominii Diversi (serve per colnames) e la matrice sendomdef finale
senderdom= character(len)
for (i in 1:len)
{
senderdom[i]=tmpdom[i]
}
sendomdef = matrix(0, nrow=nrow(datidef), ncol=len)
#creo la matrice finale
for (i in 1:nrow(sendomdef))
{
for (j in 1:length(senderdom))
{
if (datidef[i,8]== senderdom[j])
{
sendomdef[i,j]= 1
j = length(senderdom)+1
}
}
}
colnames(sendomdef)=senderdom
#SENDOMDEF MATRICE DI PRESENZA PER I DOMINII DEI SENDERS
##################################################################
#                     ANALISI DELLE FREQUENZE
##################################################################
#Avremo una matrice molto sparsa
#per avere la frequenza di ogni singola parola univoca:
freq_obj = colSums(as.matrix(objdef))
#Next, we sort this in descending order to get to know the terms with the highest frequency, as follows:
ord_obj = sort(freq_obj,decreasing=T)
top_six=(head(ord_obj)/sum(ord_obj))
barplot(ord_obj, ylab="Frequenze assolute", main="Parole ppi? frequenti nell'oggetto",ylim=(0:1), col=2:3)
##################################################################
#                     ASSEGNAZIONE SENTIMENT
##################################################################
#lo useremo come possibile predittore  futuro
sent=sentiment(datidef[,12]) #"positivo" (+1), "negativo" (-1),  "neutro" (0)
datidef=cbind(datidef,sent)
#il problema delle emoticons qui fa sbagliare qualche sent a mio avviso
prop.table(table(datidef[,16],exclude = NULL)) #ci da la proporzione di sent
barplot(table(datidef[,16]),col=2:4)
##################################################################
#
#
#
#
#                       ANALISI GRAFICA
#
#
#
#
##################################################################
##################################################################
#                     FREQUENZA PAROLE NELL'OGGETTO
##################################################################
library(ggplot2)
wf = data.frame(word=names(ord_obj), freq=ord_obj)
p = ggplot(subset(wf, freq>50), aes(word, freq)) #â™£prendiamo quelle con freq>50
p = p + geom_bar(stat="identity",color="darkblue", fill="lightblue")
p = p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
#Word Cloud
set.seed(123)
wordcloud(names(ord_obj), ord_obj, max.words=50,colors=brewer.pal(6,"Dark2"), random.order=TRUE)
#piu' scenografico. Occhio al random order 8ogni volta cambia l'ordine
##################################################################
#                     FREQUENZA SENDER
##################################################################
freq_send = colSums(sendomdef)
wf1 = data.frame(word=names(freq_send), freq=freq_send)
p1 = ggplot(subset(wf1, freq>5), aes(word, freq))
p1 = p1 + geom_bar(stat="identity",color="darkblue", fill="lightblue")
p1 = p1 + theme(axis.text.x=element_text(angle=45, hjust=1))
p1
#Word Cloud
set.seed(123)
wordcloud(names(freq_send), freq_send, max.words=50,colors=brewer.pal(6,"Dark2"), random.order=TRUE)
barplot(table(datidef[,6])/sum(freqass_y), ylab="Frequenze relative", main="Distribuzione della tipologia di email",
ylim=(0:1), col=2:4, xlab="Codifica email")
barplot(table(datidef[,11])/sum(freqass_in), ylab="Frequenze relative", main="Distribuzione tipologie di mittenti",
ylim=(0:1), col=3:5)
par(mfrow=c(1,2))
barplot(interni, ylab="Frequenze relative", main="Distr per mittenti interni",ylim=(0:1), col=2:4)
#correttamente tutte le email mandate dal dominio interno passano per il Firewall senza essere bloccate->risultato scontao
barplot(esterne, ylab="Frequenze relative", main="Distr per mittenti esterni",ylim=(0:1), col=2:4)
barplot(table(datidef[,4])/sum(freqass_fascia), ylab="Frequenze relative", main="Distribuzione delle email per fascia oraria",
ylim=(0:1), col=2:3)
par(mfrow=c(1,2))
barplot(fascia_not, ylab="Frequenze relative", main="Distr fascia notturna",ylim=(0:1), col=2:4)
barplot(fascia_lav, ylab="Frequenze relative", main="Distr fascia lavorativa",ylim=(0:1), col=2:4)
barplot(f/sum(freq_mese), ylab="Frequenze relative", main="Distribuzione email per Mese", col=2:5, ylim=c(0:1))
par(mfrow=c(2,2))
barplot(agosto, ylab="Frequenze relative", main="Distr Agosto",ylim=(0:1), col=2:4)
barplot(settembre, ylab="Frequenze relative", main="Distr Settembre",ylim=(0:1), col=2:4)
barplot(ottobre, ylab="Frequenze relative", main="Distr Ottobre",ylim=(0:1), col=2:4)
barplot(novembre, ylab="Frequenze relative", main="Distr Novembre",ylim=(0:1), col=2:4)
par(mfrow=c(1,1))
barplot(cbind(int_rel,ext_rel), ylab="Frequenze relative", main="Distr nelle email passate",ylim=(0:1), col=2:3)
wordcloud(names(ord_obj), ord_obj, max.words=50,colors=brewer.pal(6,"Dark2"), random.order=TRUE)
library(ggplot2)
library(wordcloud)
wordcloud(names(ord_obj), ord_obj, max.words=50,colors=brewer.pal(6,"Dark2"), random.order=TRUE)
#per avere la frequenza di ogni singola parola univoca:
freq_obj = colSums(as.matrix(objdef))
#se facciamo stemming
dtm = as.matrix(DocumentTermMatrix(corpus
, control = list( stemming = TRUE, stopwords = itastopwords,
minWordLength = 2, removeNumbers = TRUE,
removePunctuation = FALSE, bounds=list(local = c(1,Inf)) ))
)
##################################################################
#
#
#
#   inizio CREAZIONE DEL DIZIONARIO NEL CASO ALLDEF.CSV
#
#
#
##################################################################
make_dictionary <- function(corpus, tmp)
{
dtm = as.matrix(DocumentTermMatrix(corpus
, control = list( stemming = TRUE, stopwords = itastopwords,
minWordLength = 2, removeNumbers = TRUE,
removePunctuation = FALSE, bounds=list(local = c(1,Inf)) ))
)
coln = colnames(dtm)
dic = rep("", length(coln)+length(tmp))
for (i in 1:length(coln))
{
dic[i]=coln[i]
}
k = length(coln)+1
if(length(tmp > 0))
{
for (i in 1:length(tmp))
{
dic[k]=tmp[i]
k = k+1
}
}
return(dic)
}
n_part = 50
part = as.integer(nrow(datidef)/n_part)
tmp = rep("", 0)
for (i in 1:(n_part+1))
{
if (i <= 50)
{
if (i == 1)
{
corpus = Corpus(VectorSource(datidef[1:part*i,12]))
tmp = make_dictionary(corpus,tmp)
print(tmp)
}
else
{
corpus = Corpus(VectorSource(datidef[(part*(i-1)+1):(part*i),12]))
tmp = make_dictionary(corpus,tmp)
#print(tmp)
}
}
else
{
if (part*(i-1) < nrow(datidef))
{
corpus = Corpus(VectorSource(datidef[(part*(i-1)+1):nrow(datidef),12]))
dic = make_dictionary(corpus,tmp)
print(dic)
}
}
}
dic= wordStem(dic, language = "english")
dic= wordStem(dic, language = "italian")
dic= wordStem(dic, language = "spanish")
dic= wordStem(dic, language = "danish")
dic= wordStem(dic, language = "french")
dic= wordStem(dic, language = "german")
len = 0
idx = 1
dic=sort(dic)
colnfin = dic
#conto il numero di parole diverse in tutti gli oggetti
for (i in 2:length(dic))
{
if (colnfin[idx]!=dic[i])
{
len = len +1
idx = idx + 1
colnfin[idx]=dic[i]
}
}
#creo un vettore per memorizzare tutte le parole diverse (serve per colnames) e la matrice objdef finale
dictionary=character(len)
#inserisco tutte le parole diverse in oarine alfabetico in word
for (i in 1:len)
{
dictionary[i]= colnfin[i]
}
##################################################################
#
#
#
#   fine CREAZIONE DEL DIZIONARIO NEL CASO ALLDEF.CSV
#   presente in ***dictionary***
#
#
##################################################################
make_dictionary <- function(corpus, tmp)
{
dtm = as.matrix(DocumentTermMatrix(corpus
, control = list( stemming = TRUE, stopwords = itastopwords,
minWordLength = 2, removeNumbers = TRUE,
removePunctuation = FALSE, bounds=list(local = c(1,Inf)) ))
)
coln = colnames(dtm)
dic = rep("", length(coln)+length(tmp))
for (i in 1:length(coln))
{
dic[i]=coln[i]
}
k = length(coln)+1
if(length(tmp > 0))
{
for (i in 1:length(tmp))
{
dic[k]=tmp[i]
k = k+1
}
}
return(dic)
}
n_part = 50
part = as.integer(nrow(datidef)/n_part)
tmp = rep("", 0)
for (i in 1:(n_part+1))
{
if (i <= 50)
{
if (i == 1)
{
corpus = Corpus(VectorSource(datidef[1:part*i,12]))
tmp = make_dictionary(corpus,tmp)
print(tmp)
}
else
{
corpus = Corpus(VectorSource(datidef[(part*(i-1)+1):(part*i),12]))
tmp = make_dictionary(corpus,tmp)
#print(tmp)
}
}
else
{
if (part*(i-1) < nrow(datidef))
{
corpus = Corpus(VectorSource(datidef[(part*(i-1)+1):nrow(datidef),12]))
dic = make_dictionary(corpus,tmp)
print(dic)
}
}
}
library(SnowballC)
#(= una riga per oggetto, una colonna per ogni parola)
corpus = Corpus(VectorSource(datidef[,12]))
library(SnowballC)
library(ggplot2)
library(tm)
library(lsa)
library(caret)
library(wordcloud)
library(devtools) #installarla se necessario
library (TextWiller)
library(ggplot2)
require(tau)
#(= una riga per oggetto, una colonna per ogni parola)
corpus = Corpus(VectorSource(datidef[,12]))
##################################################################
#
#
#
#   inizio CREAZIONE DEL DIZIONARIO NEL CASO ALLDEF.CSV
#
#
#
##################################################################
make_dictionary <- function(corpus, tmp)
{
dtm = as.matrix(DocumentTermMatrix(corpus
, control = list( stemming = TRUE, stopwords = itastopwords,
minWordLength = 2, removeNumbers = TRUE,
removePunctuation = FALSE, bounds=list(local = c(1,Inf)) ))
)
coln = colnames(dtm)
dic = rep("", length(coln)+length(tmp))
for (i in 1:length(coln))
{
dic[i]=coln[i]
}
k = length(coln)+1
if(length(tmp > 0))
{
for (i in 1:length(tmp))
{
dic[k]=tmp[i]
k = k+1
}
}
return(dic)
}
n_part = 50
part = as.integer(nrow(datidef)/n_part)
tmp = rep("", 0)
for (i in 1:(n_part+1))
{
if (i <= 50)
{
if (i == 1)
{
corpus = Corpus(VectorSource(datidef[1:part*i,12]))
tmp = make_dictionary(corpus,tmp)
print(tmp)
}
else
{
corpus = Corpus(VectorSource(datidef[(part*(i-1)+1):(part*i),12]))
tmp = make_dictionary(corpus,tmp)
#print(tmp)
}
}
else
{
if (part*(i-1) < nrow(datidef))
{
corpus = Corpus(VectorSource(datidef[(part*(i-1)+1):nrow(datidef),12]))
dic = make_dictionary(corpus,tmp)
print(dic)
}
}
}
dic= wordStem(dic, language = "english")
dic= wordStem(dic, language = "italian")
dic= wordStem(dic, language = "spanish")
dic= wordStem(dic, language = "danish")
dic= wordStem(dic, language = "french")
dic= wordStem(dic, language = "german")
len = 0
idx = 1
dic=sort(dic)
colnfin = dic
#conto il numero di parole diverse in tutti gli oggetti
for (i in 2:length(dic))
{
if (colnfin[idx]!=dic[i])
{
len = len +1
idx = idx + 1
colnfin[idx]=dic[i]
}
}
#creo un vettore per memorizzare tutte le parole diverse (serve per colnames) e la matrice objdef finale
dictionary=character(len)
#inserisco tutte le parole diverse in oarine alfabetico in word
for (i in 1:len)
{
dictionary[i]= colnfin[i]
}
##################################################################
#
#
#
#   fine CREAZIONE DEL DIZIONARIO NEL CASO ALLDEF.CSV
#   presente in ***dictionary***
#
#
##################################################################
len
#
#
#
#creo la matrice finale dei domini dei sender che conta la presenza dei singoli domini
#
#
#
#
##################################################################
#ottengo tutti i dominii
len = 0
alldom=datidef[,8]
#dominii in ordine alfabetico
alldom= sort(alldom)
#conto e mi salvo tutti i dominii diversi
tmpdom = character(nrow(datidef))
idx=1
tmpdom[idx]=toString(alldom[idx])
len=1
for (i in 2:nrow(datidef))
{
if (tmpdom[idx]!=toString(alldom[i]))
{
len = len +1
idx=idx+1
tmpdom[idx]=toString(alldom[i])
}
}
#creo un vettore che contiene tutti i dominii Diversi (serve per colnames) e la matrice sendomdef finale
senderdom= character(len)
for (i in 1:len)
{
senderdom[i]=tmpdom[i]
}
sendomdef = matrix(0, nrow=nrow(datidef), ncol=len)
#creo la matrice finale
for (i in 1:nrow(sendomdef))
{
for (j in 1:length(senderdom))
{
if (datidef[i,8]== senderdom[j])
{
sendomdef[i,j]= 1
j = length(senderdom)+1
}
}
}
senderdom
length(domain)
len = 0
alldom=datidef[,8]
#dominii in ordine alfabetico
alldom= sort(alldom)
#conto e mi salvo tutti i dominii diversi
tmpdom = character(nrow(datidef))
idx=1
tmpdom[idx]=toString(alldom[idx])
len=1
for (i in 2:nrow(datidef))
{
if (tmpdom[idx]!=toString(alldom[i]))
{
len = len +1
idx=idx+1
tmpdom[idx]=toString(alldom[i])
}
}
#creo un vettore che contiene tutti i dominii Diversi (serve per colnames) e la matrice sendomdef finale
domain= character(len)
for (i in 1:len)
{
domain[i]=tmpdom[i]
}
length(domain)
memory.limit()
memory.limit(150000)
memory.limit(150000)
memory.limit()
corpus = Corpus(VectorSource(datidef[,12]))
#
# #se facciamo stemming
#
# #NB: servirebbero 140 g di memoria per lanciare questo comando->dunque cosÃ¬ completo non Ã¨ possibile lanciarlo. Capire come gestirlo.
dtm = as.matrix(DocumentTermMatrix(corpus
, control = list( stemming = TRUE, stopwords = itastopwords,
minWordLength = 2, removeNumbers = TRUE,
removePunctuation = FALSE, bounds=list(local = c(1,Inf)) ))
)
memory.limit(170000)
corpus = Corpus(VectorSource(datidef[,12]))
#
# #se facciamo stemming
#
# #NB: servirebbero 140 g di memoria per lanciare questo comando->dunque cosÃ¬ completo non Ã¨ possibile lanciarlo. Capire come gestirlo.
dtm = as.matrix(DocumentTermMatrix(corpus
, control = list( stemming = TRUE, stopwords = itastopwords,
minWordLength = 2, removeNumbers = TRUE,
removePunctuation = FALSE, bounds=list(local = c(1,Inf)) ))
)
memory.limit()
